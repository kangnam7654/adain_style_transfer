{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from model.vgg19_encoder import vgg19_encoder\n",
    "from model.vgg19_decoder import vgg19_decoder\n",
    "from utils.image_loader import image_loader\n",
    "from utils.load_transform import load_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = vgg19_encoder().to('cuda')\n",
    "decoder = vgg19_decoder().to('cuda')\n",
    "\n",
    "encoder_pt_path = './model/weights/vgg19_encoder.pt'\n",
    "decoder_pt_path = './model/weights/vgg19_decoder.pt'\n",
    "\n",
    "encoder_pt = torch.load(encoder_pt_path)\n",
    "decoder_pt = torch.load(decoder_pt_path)\n",
    "\n",
    "encoder.load_state_dict(encoder_pt, strict=False)\n",
    "decoder.load_state_dict(decoder_pt)\n",
    "\n",
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = load_transform()\n",
    "\n",
    "style_image_path = './data/picasso.jpg'\n",
    "style_image = image_loader(style_image_path, transform=transform, device=device)\n",
    "style_feature = encoder(style_image)\n",
    "style_feature = style_feature.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 64, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.calc_mean_std import calc_mean_std_\n",
    "\n",
    "style_mean, style_std = calc_mean_std_(style_feature)\n",
    "\n",
    "def adaptive_instance_normalization(content_feat=None, style_mean=None, style_std=None):\n",
    "    \"\"\"\n",
    "    논문에서 제시한 AdaIN을 구현\n",
    "    AdaIN은 content feature의 스타일을 style feature의 스타일로 변경하는 연산\n",
    "    Args:\n",
    "        content_feat (_type_): _description_\n",
    "        style_feat (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    size = content_feat.size()\n",
    "    content_mean, content_std = calc_mean_std_(content_feat)\n",
    "\n",
    "    # 평균(mean)과 표준편차(std)를 이용하여 정규화 수행\n",
    "    normalized_feat = (content_feat - content_mean.expand(size)) / content_std.expand(size)\n",
    "    # 정규화 이후에 style feature의 statistics를 가지도록 설정\n",
    "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(encoder, decoder, content_input, style_mean, style_std, alpha=1.0):\n",
    "    assert (0.0 <= alpha <= 1.0)\n",
    "    content_feature = encoder(content_input)\n",
    "    feature = adaptive_instance_normalization(content_feature, style_mean=style_mean, style_std=style_std)\n",
    "    feature = feature * alpha + content_feature * (1 - alpha) # Alpha가 1에 가까울수록 스타일이 진해짐\n",
    "    return decoder(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_process(f):\n",
    "    f = Image.fromarray(f)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cv2.VideoWriter(filename='test.mp4', fourcc=cv2.VideoWriter_fourcc(*'DIVX'), fps=25, frameSize=(512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames = []\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "assert(cap.isOpened())\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        all_frames.append(frame)\n",
    "        cv2.imshow('image', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        if len(all_frames) >= 250:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in all_frames:\n",
    "    content = frame_process(frame)\n",
    "    content = transform(content).unsqueeze(0).to('cuda')\n",
    "    result = style_transfer(encoder=encoder, decoder=decoder, content_input=content, style_mean=style_mean, style_std=style_std, alpha=0.8)\n",
    "    result = result.squeeze(0)\n",
    "    result = result.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "    out.write(result)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './data/sample.mp4'\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "assert(cap.isOpened())\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        content = frame_process(frame)\n",
    "        content = transform(content).unsqueeze(0).to('cuda')\n",
    "        out = style_transfer(encoder=encoder, decoder=decoder, content_input=content, style_mean=style_mean, style_std=style_std, alpha=0.8)\n",
    "        out = out.squeeze(0)\n",
    "        out = out.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "        out = out[:,:,::-1]\n",
    "        cv2.imshow('image', out)\n",
    "        cv2.waitKey(10)\n",
    "    else:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('style_transfer_39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0663d9bcbc7fbc469632f8a7244bc4c6aefd87758f05224b481250f615220f61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
